You are a research methodologist and data scientist documenting the VIS/SROI calculation methods for academic and practitioner audiences.

**Whitepaper Context**:
- **Company**: {{companyName}}
- **Analysis Period**: {{periodStart}} to {{periodEnd}}
- **Report Type**: Methods Whitepaper - VIS & SROI Calculation Transparency
- **Audience**: Impact evaluators, academic researchers, auditors, foundation officers, data scientists
- **Purpose**: Full transparency on how VIS (Volunteer Impact Score) and SROI (Social Return on Investment) are calculated, validated, and governed

**Calculated Metrics for This Period**:
- **SROI Ratio**: {{formatNumber sroiRatio 2}}:1
- **VIS Score**: {{formatNumber visScore 1}}/100
- **Participants**: {{participantsCount}}
- **Sessions**: {{sessionsCount}}
- **Volunteers**: {{volunteersCount}}
- **Outcome Scores** (0-1 scale):
  * Confidence: {{formatPercent avgConfidence}}
  * Belonging: {{formatPercent avgBelonging}}
  * Job Readiness: {{formatPercent avgJobReadiness}}
  * Language Level: {{formatPercent avgLanguageLevel}}
  * Well-being: {{formatPercent avgWellBeing}}

**Evidence Lineage** (Q2Q-derived data - cite with [cite:ID]):
{{#each evidenceSnippets}}
[cite:{{this.id}}] "{{this.text}}" ({{this.dimension}}, AI confidence: {{formatPercent this.score}})
{{/each}}

**Data Quality Metadata**:
{{#if dataQuality}}
- **Evidence Snippet Count**: {{dataQuality.snippetCount}}
- **Average AI Confidence**: {{formatPercent dataQuality.avgConfidence}}
- **Outcome Dimensions Covered**: {{dataQuality.dimensionsCovered}}/5
- **Data Completeness**: {{formatPercent dataQuality.completeness}}
- **Citation Density**: {{formatNumber dataQuality.citationDensity 2}} citations/100 words (in generated reports)
{{else}}
(Data quality metadata not available for this period - methods described below are standard)
{{/if}}

---

**INSTRUCTIONS**:

1. **Executive Summary** (1 paragraph, 150-180 words):
   - Purpose of this whitepaper: Methodological transparency for VIS and SROI calculations
   - Why transparency matters: Audit readiness, replicability, stakeholder trust, academic rigor
   - Key takeaways: VIS measures volunteer impact quality (0-100), SROI measures social value per dollar (ratio), both grounded in evidence
   - Data provenance: Q2Q (Qualitative-to-Quantitative) AI pipeline extracts outcome signals from participant feedback
   - Validation: Cite 1-2 evidence snippets as examples of raw data feeding calculations [cite:ID]
   - CRITICAL: Minimum 2 citations showing evidence → metric lineage

2. **Introduction: Defining VIS and SROI** (2 paragraphs, 300-350 words):
   - **Paragraph 1**: What is VIS (Volunteer Impact Score)?
     * Definition: 0-100 metric quantifying the quality (not quantity) of volunteer contributions
     * Components: Participant satisfaction with volunteers, outcome improvements attributed to volunteer support, volunteer engagement consistency
     * Why it matters: Differentiates high-impact volunteers from low-impact; guides volunteer training and matching
     * Example: VIS of {{formatNumber visScore 1}} indicates {{#if (gte visScore 80)}}exceptional{{else if (gte visScore 60)}}strong{{else}}baseline{{/if}} volunteer impact quality
     * Cite participant feedback on volunteers [cite:ID]
   - **Paragraph 2**: What is SROI (Social Return on Investment)?
     * Definition: Ratio expressing social value created per unit of investment (e.g., {{formatNumber sroiRatio 2}}:1)
     * Numerator: Monetized social value (e.g., increased earnings, reduced welfare dependency, health savings, social cohesion benefits)
     * Denominator: Total program costs (volunteer time valued, program delivery, technology, overhead)
     * Why it matters: Demonstrates economic efficiency of social programs; required for ESG reporting, foundation grants, board accountability
     * Cite evidence showing participant outcomes that feed SROI numerator [cite:ID]
   - CRITICAL: Minimum 2 citations per paragraph

3. **Data Sources & Lineage** (3 paragraphs, 400-450 words):
   - **Paragraph 1**: Raw Data Sources
     * Buddy check-in feedback (free-text qualitative responses)
     * Kintell session feedback (post-session reflections)
     * Mentor notes (volunteer observations)
     * Self-assessments (participant-reported progress)
     * All sources timestamped, linked to participants (anonymized for privacy)
     * Cite example evidence snippets from different sources (if identifiable) [cite:ID]
   - **Paragraph 2**: Q2Q AI Pipeline (Qualitative-to-Quantitative Transformation)
     * Input: Free-text participant feedback (e.g., "I feel more confident applying for jobs now")
     * Processing: NLP classification → Dimension tagging (confidence, belonging, job readiness, language, well-being)
     * Output: Evidence snippet with dimension tag, AI confidence score (0-1), timestamp, participant linkage
     * Validation: Human review of AI classifications (sample-based); inter-rater reliability checks
     * Limitations: AI may misclassify (~5-10% error rate); confidence scores reflect AI certainty, not ground truth
     * Cite evidence snippets showing AI-tagged dimensions and confidence scores [cite:ID]
   - **Paragraph 3**: Lineage Tracking (OpenLineage Events)
     * Every metric (SROI, VIS, outcome scores) traces back to source evidence snippets
     * Lineage metadata: dataset → pipeline → transformation → metric → report
     * Audit trail: Who generated? When? With what model? What data version?
     * Data residency & retention: GDPR-compliant storage, TTL policies, DSAR hooks for participant data deletion
     * Cite specific evidence IDs used in this report's calculations [cite:ID]
   - CRITICAL: Minimum 2 citations per paragraph

4. **VIS Calculation Methodology** (4 paragraphs, 500-600 words):
   - **Paragraph 1**: VIS Components Breakdown
     * Component A (40%): Participant Satisfaction with Volunteers
       - Derived from feedback mentioning volunteer support ("my buddy helped me", "I trust my mentor")
       - Sentiment analysis: Positive mentions = higher score
       - Calculation: (Positive volunteer mentions / Total volunteer mentions) × 0.4 × 100
     * Component B (40%): Outcome Improvements Attributed to Volunteers
       - Correlation analysis: Do participants with more volunteer engagement show better outcomes?
       - Attribution: Use evidence snippets explicitly crediting volunteers [cite:ID]
       - Calculation: (Avg outcome score for high-volunteer-engagement participants / Avg outcome score for low-engagement) × 0.4 × 100
     * Component C (20%): Volunteer Engagement Consistency
       - Measures volunteer retention, session attendance, response time to participant needs
       - Calculation: (Sessions with volunteer present / Total sessions scheduled) × 0.2 × 100
     * CRITICAL: Cite evidence showing volunteer-attributed outcomes [cite:ID]
   - **Paragraph 2**: VIS Formula & Calculation Example
     * Formula: VIS = (Component A × 0.4) + (Component B × 0.4) + (Component C × 0.2)
     * Example for this period:
       - Component A: {{formatNumber (multiply 0.4 visScore) 1}} (from participant satisfaction)
       - Component B: {{formatNumber (multiply 0.4 visScore) 1}} (from outcome attribution)
       - Component C: {{formatNumber (multiply 0.2 visScore) 1}} (from engagement consistency)
       - Total VIS: {{formatNumber visScore 1}}/100
     * Cite participant feedback contributing to Components A and B [cite:ID]
   - **Paragraph 3**: VIS Validation & Quality Checks
     * Face validity: Does high VIS correlate with qualitative feedback praising volunteers? (Yes - cite evidence [cite:ID])
     * Construct validity: Does VIS predict participant outcome improvements? (Test via regression analysis)
     * Reliability: Test-retest reliability over time (VIS should be stable for same volunteer pool)
     * Benchmarking: Compare VIS across cohorts, geographies, volunteer training levels
     * Limitations: VIS is a proxy; not all volunteer impact is measurable via participant feedback
     * CRITICAL: Cite evidence validating high VIS [cite:ID]
   - **Paragraph 4**: VIS Interpretation & Actionability
     * VIS 0-40: Low volunteer impact (consider volunteer training, matching improvements)
     * VIS 41-70: Moderate volunteer impact (baseline expected performance)
     * VIS 71-100: High volunteer impact (replicate volunteer practices, scale model)
     * This period's VIS ({{formatNumber visScore 1}}): {{#if (gte visScore 71)}}Exceptional - volunteer model is working well{{else if (gte visScore 41)}}Solid performance - continue current practices{{else}}Needs improvement - investigate volunteer support gaps{{/if}}
     * Cite participant voices supporting the VIS interpretation [cite:ID]
   - CRITICAL: Minimum 2 citations per paragraph

5. **SROI Calculation Methodology** (4 paragraphs, 500-600 words):
   - **Paragraph 1**: SROI Numerator - Social Value Monetization
     * Outcome 1: Increased Earnings (Job Readiness → Employment)
       - If job readiness improves by X points → Employment rate increases by Y% → Lifetime earnings gain = $Z
       - For this period: Job readiness at {{formatPercent avgJobReadiness}} → Estimated employment impact = [calculate or cite research proxy]
       - Cite evidence of job placements, CV updates, interview successes [cite:ID]
     * Outcome 2: Reduced Welfare Dependency
       - Integration → Self-sufficiency → Reduced government support costs
       - Proxy: Confidence + Job Readiness + Language → Welfare exit probability
       - Cite evidence of economic empowerment [cite:ID]
     * Outcome 3: Health Savings (Well-being → Reduced Healthcare Costs)
       - Well-being at {{formatPercent avgWellBeing}} → Mental health improvement → Reduced healthcare utilization
       - Research proxy: $X saved per point increase in well-being index
       - Cite evidence of well-being gains [cite:ID]
     * Outcome 4: Social Cohesion Benefits
       - Belonging at {{formatPercent avgBelonging}} → Community integration → Crime reduction, civic participation
       - Hard to monetize, often excluded from SROI (or use research proxies)
       - Cite evidence of social integration [cite:ID]
   - **Paragraph 2**: SROI Numerator Formula
     * Total Social Value = Σ (Outcome Improvement × Monetization Factor × Participant Count × Duration)
     * Example:
       - Job Readiness: +{{formatPercent avgJobReadiness}} × $X earnings gain × {{participantsCount}} participants = $Y
       - Well-being: +{{formatPercent avgWellBeing}} × $Z health savings × {{participantsCount}} participants = $W
       - Total Numerator: $Y + $W + ... = $TOTAL
     * Cite evidence showing outcome improvements feeding the calculation [cite:ID]
   - **Paragraph 3**: SROI Denominator - Program Costs
     * Volunteer Time: {{volunteersCount}} volunteers × Avg hours/volunteer × Volunteer hour valuation (e.g., $25-50/hr market rate)
     * Program Delivery: Staff salaries, training materials, technology platform costs
     * Overhead: Administrative costs, facilities, insurance
     * Total Denominator: $TOTAL_COSTS
     * Calculation: SROI = Total Social Value / Total Costs = {{formatNumber sroiRatio 2}}:1
     * CRITICAL: Transparent about cost assumptions (volunteer hour rate, overhead allocation)
   - **Paragraph 4**: SROI Validation & Sensitivity Analysis
     * Sensitivity test: What if volunteer hour valuation changes? ($25/hr → $50/hr: SROI shifts from X:1 to Y:1)
     * Counterfactual: What would outcomes be without the program? (Use control group if available, or research proxies)
     * Deadweight: Would some participants improve anyway (without program)? (Adjust SROI downward by deadweight %)
     * Attribution: Are outcomes solely due to this program, or also other factors? (Adjust SROI for multi-program participants)
     * This period's SROI ({{formatNumber sroiRatio 2}}:1): Conservative estimate (cite evidence of unique program contribution [cite:ID])
   - CRITICAL: Minimum 2 citations per paragraph

6. **Data Quality & Governance** (3 paragraphs, 350-400 words):
   - **Paragraph 1**: Great Expectations Test Suites
     * Automated data quality checks on all source tables (users, buddy_matches, kintell_sessions, evidence_snippets, outcome_scores)
     * Tests: Schema validation, null checks, referential integrity, numeric range checks (e.g., outcome scores 0-1)
     * Critical tables coverage: 100% (all tables feeding VIS/SROI have GE suites)
     * Test pass rate: {{#if dataQuality}}{{formatPercent dataQuality.testPassRate}}{{else}}≥90% (target){{/if}}
     * CI enforcement: `pnpm dq:ci` blocks merges if tests fail
     * CRITICAL: Data quality ensures metric reliability
   - **Paragraph 2**: Lineage & Provenance Tracking
     * OpenLineage events: Every VIS/SROI calculation emits lineage metadata
     * Traceable: Which evidence snippets → Which outcome scores → Which metrics → Which reports
     * Catalog UI: /cockpit/[companyId]/catalog shows dataset freshness, quality, lineage
     * Audit readiness: Any VIS/SROI value can be traced to source participant feedback (anonymized)
     * CRITICAL: Full auditability for ESG reporting, foundation grants, academic research
   - **Paragraph 3**: GDPR Compliance & Retention Policies
     * Data residency: EU/US/UK per tenant configuration
     * TTL policies: Evidence snippets retained for X years, then auto-deleted
     * DSAR hooks: Right to erasure - participant data deleted upon request (breaks lineage for that participant)
     * Anonymization: All evidence snippets redacted for PII before LLM generation (no participant names, emails, addresses in reports)
     * Cite example redacted evidence [cite:ID]
   - CRITICAL: Minimum 1 citation per paragraph

7. **Limitations & Future Enhancements** (2 paragraphs, 250-300 words):
   - **Paragraph 1**: Current Limitations
     * AI classification errors: ~5-10% of evidence snippets may be misclassified (dimension or confidence)
     * Counterfactual gap: No RCT control group (SROI assumes program caused all outcome improvements)
     * Monetization proxies: SROI uses research-based proxies (not actual earnings/health data for these participants)
     * Selection bias: Participants who engage more provide more feedback → May skew VIS/SROI upward
     * Missing long-term data: VIS/SROI calculated for program period only (no 1-year/5-year follow-up)
     * Cite evidence showing data gaps (e.g., limited evidence for a dimension [cite:ID])
   - **Paragraph 2**: Future Enhancements
     * Validation: Human review of AI classifications (increase sample size for inter-rater reliability)
     * RCT design: Run controlled trial to establish causal impact (compare participants vs. waitlist control)
     * Longitudinal tracking: Follow participants 1-year, 5-year post-program to measure sustained outcomes
     * External data integration: Link to employment records, health records (with consent) for actual monetization
     * dbt metrics governance: VIS/SROI calculated in dbt semantic layer (version-controlled, tested, documented)
     * Cite participant feedback requesting long-term support (evidence of future needs [cite:ID])
   - CRITICAL: Minimum 1 citation per paragraph

8. **Conclusion & Transparency Commitment** (2 paragraphs, 200-250 words):
   - **Paragraph 1**: Summary of Methods
     * VIS ({{formatNumber visScore 1}}/100): Measures volunteer impact quality via participant satisfaction, outcome attribution, engagement consistency
     * SROI ({{formatNumber sroiRatio 2}}:1): Measures social value per dollar via outcome monetization (earnings, health, cohesion) divided by program costs
     * Both metrics grounded in participant feedback (evidence snippets) extracted via Q2Q AI pipeline
     * Full lineage tracking: Every metric → source data → OpenLineage events → audit trail
     * Cite flagship evidence snippets underpinning this period's calculations [cite:ID]
   - **Paragraph 2**: Transparency & Replicability
     * This whitepaper enables: Audit verification, academic peer review, foundation due diligence, ESG reporting
     * Replicability: Methods documented, formulas published, data quality enforced, lineage tracked
     * Open questions: Counterfactual estimation, AI classification accuracy, long-term impact measurement
     * Continuous improvement: We refine VIS/SROI methods based on research, stakeholder feedback, data quality improvements
     * Contact: For methodology questions, data access requests (anonymized), or collaboration inquiries
     * CRITICAL: Cite 1-2 key evidence snippets to close with impact [cite:ID]
   - CRITICAL: Minimum 2 citations per paragraph

---

**OUTPUT REQUIREMENTS**:
- **Total Length**: 2500-3000 words
- **Tone**: Academic rigor, methodological transparency, data scientist-to-data scientist
- **Format**: Plain text with clear section headers (use "# Section Name")
- **Citations**: Inline [cite:ID] format, minimum 2 per paragraph (more in calculation sections)
- **Evidence Density**: High - every formula component linked to source data
- **Transparency**: Full disclosure of assumptions, limitations, proxies
- **No PII**: Evidence is redacted, maintain anonymity
- **Formulas**: Clearly state VIS and SROI formulas with component breakdowns
- **Tables**: Flag where data quality tables should go (e.g., "[TABLE: GE Test Suite Coverage]", "[TABLE: Lineage Metadata]")

**CRITICAL RULES**:
✅ This is a technical whitepaper - formulas, validation, limitations must be explicit
✅ Every calculation step must cite source evidence [cite:ID] showing data provenance
✅ Do not fabricate data - if a component is unavailable, state "data not available for this period"
✅ Plain text only (no markdown)
✅ Section headers with "# " prefix
✅ Transparency on AI errors, counterfactual gaps, monetization proxies
✅ This document should satisfy an auditor's due diligence requirements

**TABLE AUTO-GENERATION INSTRUCTIONS** (for downstream rendering):
The following tables will be automatically generated and inserted:
1. [TABLE: Evidence Snippet Statistics] - Count, avg confidence, dimension coverage, data completeness
2. [TABLE: GE Test Suite Coverage] - Critical tables, test count, pass rate, last run timestamp
3. [TABLE: VIS Component Breakdown] - Components A/B/C with scores and weights
4. [TABLE: SROI Numerator Breakdown] - Each outcome × monetization factor × participants = value
5. [TABLE: SROI Denominator Breakdown] - Volunteer time, delivery costs, overhead = total cost
6. [TABLE: Lineage Metadata] - Dataset → Pipeline → Metric → Report with timestamps and versions
7. [TABLE: Data Quality SLOs] - Freshness <24h, Test pass ≥90%, Lineage coverage ≥90% (current vs. target)

These will be inserted at logical points in the narrative (after relevant sections).

---

Generate the Methods Whitepaper now:
